{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "X8QKQM8dij1Q"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.signal import convolve, gaussian"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import io\n",
        "import base64\n",
        "import time\n",
        "import glob\n",
        "from IPython.display import HTML"
      ],
      "metadata": {
        "id": "r-6DA6xijIkV"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gym.wrappers import AtariPreprocessing\n",
        "from gym.wrappers import FrameStack\n",
        "from gym.wrappers import TransformReward"
      ],
      "metadata": {
        "id": "kvibQWcCjLgL"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_env(env_name, clip_rewards = True, seed = None):\n",
        "\t# complete this function which returns an object 'env' using gym module\n",
        "\t# Use AtariPreprocessing, FrameStack, TransformReward(based on the clip_rewards variable passed in the arguments of the function), check their usage from internet\n",
        "\t# Use FrameStack to stack 4 frames\n",
        "\t# TODO\n",
        "\tpass\n"
      ],
      "metadata": {
        "id": "Lm_kmR4EjOIp"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialise a device based on 'cuda' or 'cpu' which ever u have support for\n",
        "device = 'cpu' # TODO"
      ],
      "metadata": {
        "id": "oLUrfNhsjRdN"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Next we create a class DQNAgent which is the class containing the neural network, This class is derived from nn.Module\n",
        "\n",
        "class DQNAgent(nn.Module):\n",
        "\tdef __init__(self, state_shape, n_actions, epsilon):\n",
        "\t\t# TODO\n",
        "\t\t# Here state_shape is the input shape to the neural network.\n",
        "\t\t# n_Actions is the number of actions\n",
        "\t\t# epsilon is the probability to explore, 1-epsilon is the probabiltiy to stick to the best actions\n",
        "\t\t# initialise a neural network containing the following layers:\n",
        "\t\t# 1)a convulation layer which accepts size = state_shape, in_channels = 4( state_shape is stacked with 4 frames using FrameStack ), out_channels = 16, kernel_size = 8, stride = 4 followed by ReLU activation\n",
        "\t\t# 2)a convulation layer, in_channels = 16, out_channels = 32, kernel_size = 4, stride = 2 followed by ReLU activation function\n",
        "\t\t# 3)layer to convert the output to a 1D output which is fed into a linear Layer with output size = 256 followed by ReLU actiovation\n",
        "\t\t# 4) linear Layer with output size = 'number of actions'(the qvalues of actions)\n",
        "\t\tpass\n",
        "\n",
        "\tdef forward(self, state_t):\n",
        "\t\t# TODO\n",
        "\t\t# return qvalues generated from the neural network\n",
        "\t\tpass\n",
        "\n",
        "\tdef get_qvalues(self, state_t):\n",
        "\t\t# TODO\n",
        "\t\t# returns the numpy array of qvalues from the neural network\n",
        "\t\tpass\n",
        "\n",
        "\tdef sample_actions(self, qvalues):\n",
        "\t\t#TODO\n",
        "\t\t# sample_Actions based on the qvalues\n",
        "\t\t# Use epsilon for choosing between best possible current actions of the give batch_size(can be found from the qvalues object passed in argument) based on qvalues vs explorations(random action)\n",
        "\t\t# return actions\n",
        "\t\tpass\n",
        "\n"
      ],
      "metadata": {
        "id": "QjuuuHz-jViF"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(env, agent, n_games = 1, greedy = False, t_max = 10000):\n",
        "\t# used for evaluationing the trained agent for number of games = n_games and step in each game = t_max\n",
        "\t# returns the mean of sum of all rewards across n_games\n",
        "\t#TODO\n",
        "\tpass\n",
        ""
      ],
      "metadata": {
        "id": "z4PcJ9nrjZCh"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayBuffer:\n",
        "\tdef __init__(self, size):\n",
        "\t\t#TODO\n",
        "\t\t# size is the maximum size that the buffer can hold\n",
        "\t\tpass\n",
        "\n",
        "\tdef __len__(self):\n",
        "\t\t# no need to change\n",
        "\t\treturn len(self.buffer)\n",
        "\n",
        "\tdef add(self, state, action ,reward, next_state, done):\n",
        "\t\t#TODO\n",
        "\t\t# store the information passed in one call to add as 1 unit of informmation\n",
        "\t\tpass\n",
        "\n",
        "\tdef sample(self, batch_size):\n",
        "\t\t#TODO\n",
        "\t\t# return a random sampling of 'batch_size' units of information\n",
        "\t\tpass"
      ],
      "metadata": {
        "id": "0Zs0lBzIjcNQ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def play_and_record(start_state, agent, env, exp_replay, n_steps = 1):\n",
        "\t# use this function to make the agent play on the env and store the information in exp_replay which is an object of class ReplayBuffer\n",
        "\t# n_steps is the number of steps to be played in this function on one call\n",
        "\t#TODO\n",
        "\tpass\n",
        ""
      ],
      "metadata": {
        "id": "RmjtYCUUjjY0"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_td_loss(agent, target_network, device, batch_size, exp_replay,gamma = 0.99):\n",
        "\t# Here agent is the one playing on the game and target_network is updates using agent after some fixed steps as is done in Deep Q Learning\n",
        "\t# sample 'batch_size' units of info stored in the exp_replay\n",
        "\t# Find the predicted_qvalues_of_actions using agent and target_qvalues_of_actions using target_network, find the loss based on these Mean Squared Error of these two\n",
        "\t# IMPORTANT NOTE : check the type of objects, U need to convert the actions, rewards, etc, to toch.tensors for backward propogation using pytorch\n",
        "\t#TODO\n",
        "\tpass\n",
        "\n"
      ],
      "metadata": {
        "id": "uFkZalELjmeI"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import trange\n",
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "from numpy import asarray\n",
        "from numpy import savetxt\n"
      ],
      "metadata": {
        "id": "3JWreTvwjpu9"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 108\n",
        "random.seed(108)\n",
        "np.random.seed(108)\n",
        "torch.manual_seed(108)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ieiCEon4jukI",
        "outputId": "43210edb-1f43-4184-dc71-b49fd1964eed"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7c0426fed290>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##  setup environment using make_env function defined above\n",
        "# find action_space and observation_space of the atari\n",
        "# Use env_name = \"BreakoutNoFrameskip-v4\"\n",
        "# Reset the environment before starting to train the agent and everytime the game ends (U will get a done flag which is a boolean representing whether the game has ended or not)\n",
        "\n",
        "# TODO"
      ],
      "metadata": {
        "id": "KwvxcZWjjwzh"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create agent from DQNAgent class the online network\n",
        "# create target_network from DQNAgent class is updated after some fixed steps from agent\n",
        "# Note initialise target network values from agent\n",
        "\n",
        "# TODO"
      ],
      "metadata": {
        "id": "e3QnBGgRjzgv"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# created a ReplayBuffer object and saved some information in the object by playing the agent. It is better to populate some information in the Buffer, hence this step\n",
        "#filling experience replay with some samples using full random policy\n",
        "exp_replay = ReplayBuffer(10**6)\n",
        "for i in range(4000);\n",
        "    play_and_record(state, agent, env, exp_replay, n_steps=10**2)\n",
        "    print( \"Replay Buffer : i : \", i)\n",
        "    if len(exp_replay) == 10**6:\n",
        "        break\n",
        "print(len(exp_replay))"
      ],
      "metadata": {
        "id": "mSKp5IGZj2IL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#setup some parameters for training\n",
        "timesteps_per_epoch = 2\n",
        "batch_size = 32\n",
        "\n",
        "total_steps = 2 * 10**6"
      ],
      "metadata": {
        "id": "90Qs7tUYj5Kf"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Optimizer\n",
        "# TODO - use Adam optimiser from torch with learning rate (lr) = 2*1e-5"
      ],
      "metadata": {
        "id": "FReVjdv4j71n"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#setting exploration epsilon\n",
        "start_epsilon = 0.1\n",
        "end_epsilon = 0.05\n",
        "eps_decay_final_step = 1 * 10**5\n"
      ],
      "metadata": {
        "id": "BONnhQ0qj_in"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# setup spme frequency for logginf and updating target network\n",
        "loss_freq = 20\n",
        "refresh_target_network_freq = 100\n",
        "eval_freq = 10000\n",
        "\n",
        "# to clip the gradients\n",
        "max_grad_norm = 5000"
      ],
      "metadata": {
        "id": "ZY8C1m11kB8m"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_rw_history = []\n",
        "td_loss_history = []\n",
        "\n",
        "SAVE_INTERVAL = 50000\n"
      ],
      "metadata": {
        "id": "WRZL27JekFTu"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def epsilon_schedule(start_eps, end_eps, step, final_step):\n",
        "    return start_eps + (end_eps-start_eps)*min(step, final_step)/final_step"
      ],
      "metadata": {
        "id": "raFXORN8kOnf"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO - reset the state of the environment before starting"
      ],
      "metadata": {
        "id": "nqRvVbh0kSc-"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## MAIN LOOP STARTING\n",
        "for step in trange(total_steps + 1):\n",
        "\n",
        "\t#TODO update the exploration probability (epsilon) as time passes\n",
        "\n",
        "\t#TODO taking timesteps_per_epoch and update experience replay buffer, (use play_and_record)\n",
        "\n",
        "\t#TODO compute loss\n",
        "\n",
        "\t#TODO Backward propogation and updating the network parameters\n",
        "\t# IMPORTANT NOTE : You only need to update the parameters of agent and not of target_network, that will be done according to refresh_target_network_freq. But Backward Propogation will take into account the target_network parameters as well. So use detach() method on target_network while calculating the loss. Google what it does and how to use !!\n",
        "\n",
        "\n",
        "\tif step % loss_freq == 0:\n",
        "\t\ttd_loss_history.append(loss.data.cpu().item())\n",
        "\n",
        "\n",
        "\tif step % refresh_target_network_freq == 0:\n",
        "        #TODO Load agent weights into target_network\n",
        "\n",
        "\n",
        "\tif step % eval_freq == 0:\n",
        "\t\tmean_reward = evaluate(make_env(env_name, seed=step), agent, n_games=3, greedy=True, t_max=6000)\n",
        "\t\tmean_rw_history.append(mean_reward)\n",
        "\n",
        "\t\tprint(\"mean_reward : \", mean_reward)\n",
        "\n",
        "\t\tclear_output(True)\n",
        "\t\tprint(\"buffer size = %i, epsilon = %.5f\" %\n",
        "\t\t\t\t(len(exp_replay), agent.epsilon))\n",
        "\n",
        "\n",
        "    # #Save\n",
        "    if step % SAVE_INTERVAL == 0 and step!= 0:\n",
        "        print('Saving...')\n",
        "        device = torch.device('cpu')\n",
        "        torch.save(agent.state_dict(), f'model_{step}.pth')\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        savetxt(f'reward_{step}.csv', np.array(mean_rw_history))\n",
        "\n",
        "\n",
        "\n",
        "# savetxt('reward_final.csv', np.array(mean_rw_history))"
      ],
      "metadata": {
        "id": "gMtvADJGkSf1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_score = evaluate(\n",
        "  make_env(env_name),\n",
        "  agent, n_games=1, greedy=True, t_max=10000\n",
        ")\n",
        "print('final score:', final_score)"
      ],
      "metadata": {
        "id": "A1mJxYsEkcPh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}